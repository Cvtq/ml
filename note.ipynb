{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from imblearn.over_sampling import ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/ivtipm/ML/main/datasets/churn.csv -O churn.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"churn.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Duplicates: №{df.duplicated().sum()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Классы несбалансированны"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.histplot(df['churn'], discrete=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.histplot(df['Income_Category'], discrete=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "sb.histplot( data=df, x='Customer_Age', hue='churn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "sb.histplot( data=df, x='Customer_Age', hue='Income_Category')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Клиенты с низким объемом переводов уходят чаще"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "sb.histplot( data=df, x='Total_Trans_Ct', hue='churn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numerical_cols(df: pd.DataFrame):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    num_df = df.select_dtypes(include=numerics)    \n",
    "    num_cols = num_df.columns\n",
    "    return num_cols\n",
    "\n",
    "def get_obj_cols(df: pd.DataFrame):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    obj_df = df.select_dtypes(exclude=numerics)\n",
    "    obj_cols = obj_df.columns\n",
    "    return obj_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "\n",
    "num_cols = get_numerical_cols(df)\n",
    "num_colsn_n = len(num_cols)\n",
    "\n",
    "ncols = 3\n",
    "nrows = ceil(num_colsn_n/ncols)\n",
    "\n",
    "fig, axs = plt.subplots(nrows=nrows, ncols=ncols,  figsize=(16, 16))\n",
    "for i, c in enumerate(num_cols):\n",
    "    sb.boxplot(df[c].values, ax=axs[i % nrows, i % ncols]).set_title(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.boxplot(data = df, x = 'churn', y='Total_Trans_Amt', hue='Gender')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.boxplot(data = df, x = 'churn', y='Avg_Utilization_Ratio', hue='Gender')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sb.violinplot(x=df[\"Total_Revolving_Bal\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.pairplot(df, hue='churn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sb.pairplot(df, hue='Income_Category')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Корреляция признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "sb.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt=\".2f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cl = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_cols = get_obj_cols(df_cl)\n",
    "for c in obj_cols:\n",
    "    print(f\"Unique values of {c}: {df_cl[c].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # удалим неизвестные категории доходов\n",
    "# df_cl = df_cl[df_cl['Income_Category'] != 'Unknown']\n",
    "\n",
    "# # удалим неизвестный семейный статус\n",
    "# df_cl = df_cl[df_cl['Marital_Status'] != 'Unknown']\n",
    "\n",
    "# # удалим неизвестный семейный статус\n",
    "# df_cl= df_cl[df_cl['Marital_Status'] != 'Unknown']\n",
    "\n",
    "# # удалим неизвестный уровень образования\n",
    "# df_cl = df_cl[df_cl['Education_Level'] != 'Unknown']\n",
    "\n",
    "df_cl = df_cl.drop(columns=['Education_Level'])\n",
    "\n",
    "obj_cols = get_obj_cols(df_cl)\n",
    "for c in obj_cols:\n",
    "    print(f\"Unique values of {c}: {df_cl[c].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cols_to_drop=['Months_on_book', 'Credit_Limit', 'Total_Revolving_Bal', 'Total_Trans_Ct', 'Avg_Open_To_Buy']\n",
    "cols_to_drop=['Months_on_book', 'Credit_Limit', 'Total_Revolving_Bal']\n",
    "df_cl.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "sb.heatmap(df_cl.corr(), annot=True, cmap='coolwarm', fmt=\".2f\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, Normalizer, LabelBinarizer, LabelEncoder, Normalizer, RobustScaler, QuantileTransformer, minmax_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_enc = df_cl.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Энкодинг нескольких классов\n",
    "le = LabelEncoder()\n",
    "df_enc['Income_Category'] = le.fit_transform(df_enc['Income_Category'])    \n",
    "\n",
    "# Энкодинг двух классов\n",
    "lb = LabelBinarizer()\n",
    "df_enc['churn'] = lb.fit_transform(df_enc['churn'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_cols = get_obj_cols(df_enc)\n",
    "\n",
    "ohe_cols = []\n",
    "# OHE на другие признаки\n",
    "for c in obj_cols:\n",
    "    ohe = OneHotEncoder()\n",
    "    t = ohe.fit_transform(df_enc[[c]])    \n",
    "    df_enc[ohe.categories_[0]] = t.toarray()\n",
    "    ohe_cols += list(ohe.categories_[0])\n",
    "\n",
    "# Удалим obj cols после энкодинга    \n",
    "df_enc.drop(columns=obj_cols, inplace=True)\n",
    "df_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_cols += ['churn', 'Income_Category']\n",
    "ohe_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_cols =  list(set(df_enc.columns.tolist()) - set(ohe_cols))\n",
    "transform_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qt = QuantileTransformer()      \n",
    "df_enc[transform_cols] = qt.fit_transform(df_enc[transform_cols])      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rs = RobustScaler()      \n",
    "# df_enc[transform_cols] = rs.fit_transform(df_enc[transform_cols])      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = Normalizer(norm='max')      \n",
    "# df_enc[ohe_cols] = n.fit_transform(df_enc[ohe_cols])      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_enc[ohe_cols] = minmax_scale(df_enc[ohe_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = StandardScaler()  \n",
    "# df_enc[transform_cols] = scaler.fit_transform(df_enc[transform_cols])      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_enc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_label = 'churn'\n",
    "\n",
    "Y = df_enc[prediction_label]\n",
    "X = df_enc.drop(columns=[prediction_label]) \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.2, random_state=42, shuffle=True)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Original dataset shape %s' % Counter(Y))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# балансировка классов через уменьшение перепредставленного класса\n",
    "msk_negative = df_enc[prediction_label] == 1\n",
    "msk_positive = df_enc[prediction_label] == 0\n",
    "\n",
    "df_negative_undersample = df_enc[msk_negative].sample(n=msk_positive.sum(), random_state=888)\n",
    "df_undersample = pd.concat([df_negative_undersample, df_enc[msk_positive]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uY = df_undersample[prediction_label]\n",
    "uX = df_undersample.drop(columns=[prediction_label]) \n",
    "\n",
    "uX_train, uX_test, uy_train, uy_test = train_test_split(uX, uY, test_size=0.2, random_state=42, shuffle=True)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Undersampled dataset shape %s' % Counter(uY))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada = ADASYN(random_state=42)\n",
    "X_res, y_res = ada.fit_resample(X, Y)\n",
    "\n",
    "resX_train, resX_test, resy_train, resy_test = train_test_split(X_res, y_res, test_size=0.2, random_state=42, shuffle=True)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Resampled dataset shape %s' % Counter(y_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 42, shuffle=True)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = ((X, Y, \"Base\"), (uX, uY, \"Undersampled\"), (X_res, y_res, \"Oversampled\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, f1_score, fbeta_score, roc_auc_score, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pair in pairs:\n",
    "    X_train, X_test, y_train, y_test = split(pair[0], pair[1])\n",
    "    lr_model = LogisticRegression(max_iter=200)\n",
    "    lr_model.fit(X_train, y_train)\n",
    "    y_pred = lr_model.predict(X_test)\n",
    "    y_pred_train = lr_model.predict(X_train)        \n",
    "    \n",
    "    if False: # print train or not\n",
    "        print(f\"{'='*10} {pair[2]} Train {'='*10}\")\n",
    "        print(f\"Accuracy {accuracy_score(y_train, y_pred_train)} F1: {f1_score(y_train, y_pred_train)}\")\n",
    "        print(\"Confusion Matrix \\n\", confusion_matrix(y_train, y_pred_train))\n",
    "        print(\"Classification Report \\n\", classification_report(y_train, y_pred_train))    \n",
    "        \n",
    "    print(f\"{'='*10} {pair[2]} Test {'='*10}\")\n",
    "    print(f\"Accuracy {accuracy_score(y_test, y_pred)} F1: {f1_score(y_test, y_pred)}\")\n",
    "    print(\"Confusion Matrix \\n\", confusion_matrix(y_test, y_pred))    \n",
    "    print(\"Classification Report \\n\", classification_report(y_test, y_pred))    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import absolute\n",
    "from sklearn.metrics import r2_score\n",
    "from numpy import arange\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_cross_val_score(model, scorings):    \n",
    "    for pair in pairs:    \n",
    "        print(f\"{'='*5} Result on {pair[2]} dataset:\")\n",
    "        for scoring in scorings:            \n",
    "            cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=0)\n",
    "            scores = cross_val_score(model, pair[0], pair[1], scoring=scoring, cv=cv, n_jobs=-1)\n",
    "            scores = absolute(scores)                    \n",
    "            print(f\"{scoring}: mean {mean(scores):1.4f} std {std(scores):1.4f}\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(model_class, X, y, scoring, grid):\n",
    "    model = model_class()    \n",
    "    cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=0)    \n",
    "    #grid['max_iter'] = arange(100, 1000, 50)    \n",
    "    search = GridSearchCV(model, grid, scoring=scoring, cv=cv, n_jobs=-1)\n",
    "    results = search.fit(X, y)\n",
    "    print(f\"Best {scoring} score: {results.best_score_:1.4f}\")\n",
    "    print(f\"Config: {results.best_params_}\")\n",
    "    return results.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain, combinations\n",
    "\n",
    "def powerset(iterable):\n",
    "    \"powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)\"\n",
    "    s = list(iterable)  # allows duplicate elements\n",
    "    return chain.from_iterable(combinations(s, r) for r in range(len(s)+1))\n",
    "\n",
    "def grid_search_drops(model_class, X, y, scoring):\n",
    "    cols_to_drop=X.columns    \n",
    "    best_score = -1\n",
    "    best_conf = {}    \n",
    "    best_drop = []\n",
    "    for i, combo in enumerate(powerset(cols_to_drop), 1):    \n",
    "        if i % 50 == 0:\n",
    "            print('Testing combo #{}: {}'.format(i, list(combo)))\n",
    "        _X = X.drop(columns=list(combo))\n",
    "        model = model_class()    \n",
    "        cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=0)    \n",
    "        grid = dict()\n",
    "        grid['alpha'] = arange(0.001, 1, 0.2)          \n",
    "        search = GridSearchCV(model, grid, scoring=scoring, cv=cv, n_jobs=-1)\n",
    "        results = search.fit(_X, y)\n",
    "        if best_score < results.best_score_:\n",
    "            best_score = results.best_score_\n",
    "            best_conf = results.best_params_\n",
    "            best_drop = list(combo)                        \n",
    "    print(f\"Best {scoring} score: {best_score:1.4f}\")\n",
    "    print(f\"Config: {results.best_params_}\")\n",
    "    print(f\"Best Drop: {best_drop}\")            \n",
    "    return best_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_grid = dict()\n",
    "reg_grid['alpha'] = arange(0.001, 1, 0.05)\n",
    "reg_scorings = ['neg_mean_absolute_error', 'r2']\n",
    "reg_scoring = 'r2'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1\n",
    "$ L_{reg} = L(b) +\\lambda \\sum \\limits_{j}^{n}b_j^2. $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pair in pairs:    \n",
    "    print(f\"{'*'*10} Grid Search on {pair[2]} dataset {'*'*10}\")\n",
    "    params = grid_search(Lasso, pair[0], pair[1], scoring=reg_scoring, grid=reg_grid)\n",
    "    model_cross_val_score(Lasso(**params), scorings=reg_scorings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2\n",
    "$ L_{reg}  = L(b)+\\lambda \\sum \\limits_{i}^{n} |b| $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pair in pairs:    \n",
    "    print(f\"{'*'*10} Grid Search on {pair[2]} dataset {'*'*10}\")\n",
    "    params = grid_search(Ridge, pair[0], pair[1], scoring=reg_scoring, grid=reg_grid)\n",
    "    model_cross_val_score(Ridge(**params), scorings=reg_scorings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ElasticNet\n",
    "$ L_{reg}= L(b) +\\lambda_1 \\sum \\limits_{i}^{n} |b|  + \\lambda_2 \\sum \\limits_{j}^{n}b_j^2. $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pair in pairs:    \n",
    "    print(f\"{'*'*10} Grid Search on {pair[2]} dataset {'*'*10}\")\n",
    "    params = grid_search(ElasticNet, pair[0], pair[1], scoring=reg_scoring, grid=reg_grid)\n",
    "    model_cross_val_score(ElasticNet(**params), scorings=reg_scorings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_grid = dict()\n",
    "knn_grid['n_neighbors'] = arange(1, 50, 2)\n",
    "knn_scorings = ['f1', 'accuracy']\n",
    "knn_scoring = 'f1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pair in pairs:    \n",
    "    print(f\"{'*'*10} Grid Search on {pair[2]} dataset {'*'*10}\")\n",
    "    params = grid_search(KNeighborsClassifier, pair[0], pair[1], scoring=knn_scoring, grid=knn_grid)\n",
    "    model_cross_val_score(KNeighborsClassifier(**params), knn_scorings)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_grid = dict()\n",
    "svc_grid['C'] = arange(2, 6, 0.5)\n",
    "svc_grid['kernel'] = ['linear'] #, 'poly', 'rbf', 'sigmoid', 'precomputed'\n",
    "svc_scorings = ['f1', 'accuracy']\n",
    "svc_scoring = 'f1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pair in pairs:    \n",
    "    print(f\"{'*'*10} Grid Search on {pair[2]} dataset {'*'*10}\")\n",
    "    params = grid_search(SVC, pair[0], pair[1], scoring=svc_scoring, grid=svc_grid)\n",
    "    model_cross_val_score(SVC(**params), svc_scorings)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
